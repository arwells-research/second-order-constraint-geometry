\section{Relation to the Quantum Formalism}
\label{sec:comparison-quantum-formalism}

This section clarifies the relationship between second-order constraint geometry
$\Sigma_2$ and the standard quantum-mechanical formalism. The aim is neither to
revise nor to extend quantum mechanics, but to make explicit a structural feature
of its operational use: the reliance on admissibility constraints that are
history-dependent, even when expressed in state-based language.

Nothing in this section asserts a failure of quantum mechanics. The role of
$\Sigma_2$ is diagnostic and classificatory. It distinguishes structure that is
intrinsically state-local from admissibility relations that depend on ordered
history and are therefore not representable at the level of instantaneous state
alone.

\subsection{State sufficiency in the quantum postulates}

The quantum postulates assert that the physical state of a system at an instant of
time is fully specified by a state vector or density operator, and that future
measurement statistics are determined by this state together with the governing
Hamiltonian. Formally, this constitutes a first-order \emph{predictive} sufficiency claim: given $\rho(t)$, future statistics are fixed.

This claim is exact within the formal structure of the theory. However, the
operational deployment of quantum mechanics routinely invokes constraints that
are not encoded solely in the instantaneous state. These constraints become
visible when one asks not only what statistics will occur, but which operations,
extensions, or reversals are physically admissible.

\subsection{Preparation procedures and admissible histories}

Quantum predictions are conditioned on preparation procedures. Two density
operators may be mathematically identical while corresponding to preparation
histories that are not operationally interchangeable. Distinctions between
post-selected ensembles, decohered subsystems, and directly prepared states are
often suppressed once $\rho$ is specified, but reappear when considering which
future operations or embeddings remain admissible.

This indicates that admissibility is not exhausted by instantaneous state. The
quantum formalism remains predictively sufficient, but the admissible-history
structure on which those predictions rely is not fully represented at the state
level.

\subsection{Measurement update as a boundary-conditioned operation}

The measurement update rule introduces a non-unitary state change conditioned on a
specific outcome. This update is anchored at a definite boundary: the measurement
event. While the post-measurement state suffices for predicting subsequent outcome
statistics, the admissibility of the update itself presupposes an ordered history
that includes interaction, outcome registration, and conditioning.

From the perspective of $\Sigma_2$, this reflects an implicit boundary-conditioned
admissibility structure. Measurement theory operates successfully with this
structure in place, even though it is not represented explicitly as part of the
state description.

\subsection{Decoherence and environmental tracing}

Decoherence theory accounts for the suppression of interference by tracing over
environmental degrees of freedom. The resulting reduced density matrix often
appears diagonal in a preferred basis. However, the reduced state alone does not
encode whether coherence was lost through interaction or was never present. This
distinction is immaterial for many predictions, but becomes decisive when
considering reversibility, recoherence, or extension to a larger system.

The need to reference interaction history rather than instantaneous state is a
diagnostic signature of second-order constraint structure. It does not signal a
deficiency of the theory, but a limit of state-based representation.

\subsection{Unitary equivalence and admissible extension}

Quantum mechanics identifies states up to unitary equivalence. Unitary equivalence,
however, does not guarantee equivalence of admissible continuations. Reduced states
that are unitarily equivalent may differ in whether they admit consistent
reversal, extension to a larger Hilbert space, or recombination with previously
correlated subsystems.

Such distinctions do not contradict the formalism. They indicate that
admissibility relations are defined over histories and extensions, rather than
over instantaneous states alone.

\subsection{Structural clarification}

Second-order constraint geometry introduces no new variables, modifies no quantum
dynamics, and reinterprets no probabilities. It provides a language for
classifying when the successful use of quantum mechanics relies purely on state
sufficiency and when it relies on additional admissibility constraints introduced
by preparation, measurement, or reduction.

Recognizing these constraints explicitly sharpens the distinction between formal
predictive sufficiency and representational completeness. Quantum mechanics
remains correct and empirically validated; $\Sigma_2$ identifies the structural
regime in which state-based descriptions reach their natural limit.

\subsection{Relation to Consistent and Decoherent Histories}
\label{subsec:consistent-histories-relation}

The present framework bears a superficial resemblance to consistent and
decoherent histories (CH/DH) approaches, in that both treat histories rather than
instantaneous states as conceptually significant. It is therefore important to
clarify the precise relation and points of distinction.

Consistent histories formulations take histories as fundamental objects and
introduce consistency or decoherence conditions to ensure that probabilities can
be assigned to sets of histories without contradiction. These conditions restrict
which collections of histories may be treated as mutually exclusive and jointly
exhaustive alternatives. The primary aim is to extend probabilistic reasoning
beyond single-time measurements while retaining internal coherence of the
probability calculus.

Second-order constraint geometry addresses a different question. It does not
propose a new history-based formulation of quantum mechanics, nor does it impose
consistency conditions or define probability assignments over histories. Instead,
$\Sigma_2$ provides a diagnostic criterion for when state-based representations
fail to be faithful to admissible-history structure, independently of whether
probabilities over those histories are well defined.

A system may therefore satisfy the consistency criteria of the histories
framework while still failing to admit a faithful state-based representation of
admissibility in the sense of
Definition~\ref{def:faithful-state-representation}. In this sense, consistent
histories and $\Sigma_2$ are complementary but non-competing.

\subsection{Relation to Existing Literature}
\label{subsec:relation-existing-literature}

The perspective developed here intersects with several established frameworks in
quantum foundations, though its aims and conclusions are distinct. We briefly
situate second-order constraint geometry ($\Sigma_2$) relative to these approaches
to clarify both points of contact and departure.

\paragraph{Consistent and decoherent histories.}
The consistent histories program, introduced by Griffiths and further developed
by Omnès and others, treats histories as primary objects and imposes consistency
conditions to ensure that probabilities can be coherently assigned to sets of
histories \cite{griffiths1984consistent, griffiths2002consistent}. These
conditions restrict interference between histories so that classical probability
logic applies.

Second-order constraint geometry addresses a different question. It does not
impose consistency conditions, nor does it assign probabilities to histories.
Instead, $\Sigma_2$ diagnoses when admissibility of future operations fails to
factorize through instantaneous state, even when probabilities are well defined.

\paragraph{Decoherence and einselection.}
Decoherence theory, particularly as developed by Zurek, explains the emergence of
classical behavior via environmentally induced suppression of interference and
environment-induced superselection (einselection)
\cite{zurek1991decoherence, zurek2003decoherence}. These mechanisms operate
dynamically and statistically, identifying robust pointer states.

The present framework is orthogonal to these mechanisms. $\Sigma_2$ does not
address which states are dynamically selected, but whether admissibility of
operations can be represented faithfully at the level of instantaneous state.

\paragraph{Foundational no-go theorems.}
Results such as Bell’s theorem \cite{bell1964einstein} and the
Kochen--Specker theorem \cite{kochen1967problem} constrain state-based extensions
of quantum mechanics. Second-order constraint geometry does not propose such
extensions. It clarifies a different limitation: the failure of state-based
descriptions to encode admissibility relations under symmetry-preserving
representation, even when predictive completeness is preserved.

\paragraph{Trajectory-based and path-integral approaches.}
Trajectory-based and path-integral formulations emphasize histories as
computational tools. While they make essential use of trajectories, they do not
typically address when admissibility itself becomes history-dependent in a way
that cannot be compressed into state variables.

$\Sigma_2$ is agnostic to computational formalism. Its contribution is to identify
when history-level constraints are structurally irreducible under faithful,
symmetry-preserving state representation, regardless of the representational
technique employed.